### 用以下seq2seq翻译模型来说明注意力机制：
![](https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_720w.jpg)

比如现在要翻译第一个位置的词即y1
没用注意力机制的话就直接对h1'接一个全连接网络
用了注意力机制会先生成一个中间状态h1''，然后一样再接全连接做分类，h1''就是注意力机制的结果，也就是下图公式的结果
![](https://img-blog.csdnimg.cn/20210518151919234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNTE4NzQ0,size_16,color_FFFFFF,t_70)

此时，Q就是h1'，K和V可以直接是[h1, h2, h3, h4]，或者是对[h1, h2, h3, h4]做全连接后的结果。
Q*K的结果是一个分数比如[0.1, 0.2, 0.6, 0.1]，根据向量点积原理，两个向量越相关点积越大 ，那该结果就说明该位置的翻译结果跟h3的关系是最大的，
最后乘V的意思是h1,h2,h3,h4都要考虑，只是会分配不同的注意力分数